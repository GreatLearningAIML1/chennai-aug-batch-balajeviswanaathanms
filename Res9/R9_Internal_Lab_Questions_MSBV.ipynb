{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mesyLlOGC4Lm"
   },
   "source": [
    "Dear Participant,\n",
    "\n",
    "Please find below the the files required to complete the internal lab assessment.\n",
    "\n",
    "R9_Internal_Lab_Questions.ipynbView in a new window\n",
    "\n",
    "Data Set:\n",
    "\n",
    "images-20190421T070243Z-001.zipView in a new window\n",
    "\n",
    "daily-minimum-temperatures-in-me.csvView in a new window\n",
    "\n",
    "train_labels.csvView in a new window\n",
    "\n",
    "\n",
    "Below is the Github link to accept the assignment. \n",
    "\n",
    "Github Link: https://classroom.github.com/a/WSWMHTtW\n",
    "\n",
    "Kindly refer the Github module before uploading your submission.\n",
    "\n",
    "The final submission of this lab assessment should be made on Olympus. Please submit the Github link of your work.\n",
    "\n",
    "Please note: Since now, you will be pushing all your assessment files to the same repository for the remainder of the program, so it is important that you follow some name structure to identify your assessment submission properly.\n",
    "\n",
    "Please name your file in the following format - ResidencyNumber_TypeofLab.ipynb\n",
    "\n",
    "Ex -R9_InternalLab.ipynb\n",
    "\n",
    "Regards\n",
    "\n",
    "Program Office"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1xi6A_naxGb"
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlWkRaF0axTl"
   },
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES AND PACKAGES\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from keras import Model\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from keras.layers import Conv2D, Reshape\n",
    "from keras.utils import Sequence\n",
    "from keras.backend import epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6Xs5k8iKoFe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paKO7ERla7Hk"
   },
   "source": [
    "### Load the training data from train.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1B0XW8oOa7Xi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPsbqQ43NGPl"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svojxm_jNIPt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbA5FUnZNU9W"
   },
   "outputs": [],
   "source": [
    "DATASET_FOLDER = \"/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab/\"\n",
    "\n",
    "images_zip_path = DATASET_FOLDER + \"images.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvSWH7WyNZGV"
   },
   "outputs": [],
   "source": [
    "images_zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvXfj3chNipd"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(images_zip_path, 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xLMe6DyOzDA"
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = pd.read_csv(\"/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UweyCk1a-73"
   },
   "source": [
    "### Print the shape of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zb-bC1sRUSSg"
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ulTUxHwa_KO"
   },
   "outputs": [],
   "source": [
    "print(TRAIN_CSV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EmVHq8PbEGU"
   },
   "source": [
    "### Declare a variable IMAGE_SIZE = 128 as we will be using MobileNet which will be taking Input shape as 128 * 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAZhqikIbEQz"
   },
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "\n",
    "ALPHA = 1.0 # Width hyper parameter for MobileNet (0.25, 0.5, 0.75, 1.0). Higher width means more accurate but slower\n",
    "\n",
    "IMAGE_SIZE = 128 # MobileNet takes images of size 128*128*3 \n",
    "\n",
    "EPOCHS = 10 # Number of epochs. I got decent performance with just 5.\n",
    "BATCH_SIZE = 32 # Depends on your GPU or CPU RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xCLu4gDbKV0"
   },
   "source": [
    "### With the help of csvr.reader write a for loop which can load the train.csv file and store the path, width, height, x0,y0,x1,y1 in induvidual variables. <br>\n",
    "1. Create a list variable known as 'path' which has all the path for all the training images\n",
    "2. Create an array 'coords' which has the resized coordinates of the bounding box for the training images\n",
    "\n",
    "<u>Note:</u> All the training images should be downsampled to 128 * 128 as it is the input shape of MobileNet (which we will be using for Object detection). Hence the corresponding coordinates of the bounding boxes should be changed to match the image dimension of 128 * 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_zaAPRHUqRB"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErCgIbtQSNVX"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab/train_labels.csv', 'r') as csvfile:\n",
    "    paths = []\n",
    "    coords = np.zeros((sum(1 for line in csvfile)-1, 4))\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    csvfile.seek(0)\n",
    "    \n",
    "    is_first_row = True\n",
    "    for col, row in enumerate(reader):\n",
    "        if is_first_row:\n",
    "          is_first_row = False\n",
    "          continue\n",
    "          \n",
    "        for i, r in enumerate(row[1:8]): # Parse row with seven entities\n",
    "#          print(i, r)\n",
    "          if i == 2:\n",
    "            continue\n",
    "          else:\n",
    "            row[i+1] = int(r)\n",
    "              \n",
    "        path, image_width, image_height, _, x0, y0, x1, y1 = row  \n",
    "        path = \"/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab/images/images/\" + path\n",
    "        coords[col-1, 0] = x0 * IMAGE_SIZE / image_width # Normalize bounding box by image size\n",
    "        coords[col-1, 1] = y0 * IMAGE_SIZE / image_height # Normalize bounding box by image size\n",
    "        coords[col-1, 2] = (x1 - x0) * IMAGE_SIZE / image_width # Normalize bounding box by image size\n",
    "        coords[col-1, 3] = (y1 - y0) * IMAGE_SIZE / image_height\n",
    "        paths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CMdmgbvbuXg"
   },
   "source": [
    "### Write a for loop which can load all the training images into a variable 'batch_images' using the paths from the 'paths' variable\n",
    "<u>Note:</u> Convert the image to RGB scale as the MobileNet accepts 3 channels as inputs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLckLOBObujI"
   },
   "outputs": [],
   "source": [
    "batch_images = np.zeros((len(paths), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n",
    "for i, f in enumerate(paths):\n",
    "    img = Image.open(f) # Read image\n",
    "    img = img.resize((IMAGE_SIZE, IMAGE_SIZE)) # Resize image\n",
    "    img = img.convert('RGB')\n",
    "    batch_images[i] = preprocess_input(np.array(img, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZp1nd6_bylk"
   },
   "source": [
    "### Import MobileNet and load MobileNet into a variable named 'model' which takes input shape of 128 * 128 * 3. Freeze all the layers. Add convolution and reshape layers at the end to ensure the output is 4 coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IL7Vxs_7by0k"
   },
   "outputs": [],
   "source": [
    "model = MobileNet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, alpha=ALPHA) # Load pre-trained mobilenet\n",
    "# Do not include classification (top) layer\n",
    "\n",
    "# to freeze layers, except the new top layer, of course, which will be added below\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new top layer which is a conv layer of the same size as the previous layer so that only 4 coords of BBox can be output\n",
    "x = model.layers[-1].output\n",
    "x = Conv2D(4, kernel_size=4, name=\"coords\")(x)\n",
    "# In the line above kernel size should be 3 for img size 96, 4 for img size 128, 5 for img size 160 etc.\n",
    "x = Reshape((4,))(x) # These are the 4 predicted coordinates of one BBox\n",
    "\n",
    "model = Model(inputs=model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhXvlzVOaq1r"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hdy9wEe8b3Ub"
   },
   "source": [
    "### Define a custom loss function IoU which calculates Intersection Over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgEL4Mwub3do"
   },
   "outputs": [],
   "source": [
    "def loss(gt,pred):\n",
    "    intersections = 0\n",
    "    unions = 0\n",
    "    diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n",
    "    diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n",
    "    intersection = diff_width * diff_height\n",
    "    \n",
    "    # Compute union\n",
    "    area_gt = gt[:,2] * gt[:,3]\n",
    "    area_pred = pred[:,2] * pred[:,3]\n",
    "    union = area_gt + area_pred - intersection\n",
    "\n",
    "#     Compute intersection and union over multiple boxes\n",
    "    for j, _ in enumerate(union):\n",
    "        if union[j] > 0 and intersection[j] > 0 and union[j] >= intersection[j]:\n",
    "            intersections += intersection[j]\n",
    "            unions += union[j]\n",
    "\n",
    "    # Compute IOU. Use epsilon to prevent division by zero\n",
    "    iou = np.round(intersections / (unions + epsilon()), 4)\n",
    "    iou = iou.astype(np.float32)\n",
    "    return iou\n",
    "\n",
    "def IoU(y_true, y_pred):\n",
    "    iou = tf.py_func(loss, [y_true, y_pred], tf.float32)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCHmzs3Zb58T"
   },
   "source": [
    "### Write model.compile function & model.fit function with: <br>\n",
    "1. Optimizer = Adam, Loss = 'mse' and metrics = IoU\n",
    "2. Epochs = 30, batch_size = 32, verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV4sVCivb6FJ"
   },
   "outputs": [],
   "source": [
    "gt = coords\n",
    "model.compile(optimizer='Adam', loss='mse', metrics=[IoU]) # Regression loss is MSE\n",
    "\n",
    "#checkpoint = ModelCheckpoint(\"model-{val_iou:.2f}.h5\", verbose=1, save_best_only=True,\n",
    "#                              save_weights_only=True, mode=\"max\", period=1) # Checkpoint best validation model\n",
    "#stop = EarlyStopping(monitor=\"val_iou\", patience=PATIENCE, mode=\"max\") # Stop early, if the validation error deteriorates\n",
    "#reduce_lr = ReduceLROnPlateau(monitor=\"val_iou\", factor=0.2, patience=10, min_lr=1e-7, verbose=1, mode=\"max\")\n",
    "# Reduce learning rate if Validation IOU does not improve\n",
    "\n",
    "model.fit(batch_images,gt,\n",
    "            epochs=30,batch_size = 32,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWrj3s6Rb-C8"
   },
   "source": [
    "### Pick a test image from the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5zeEuTFb-M0"
   },
   "outputs": [],
   "source": [
    "# Pick a test image, run model, show image, and show predicted bounding box overlaid on the image\n",
    "import cv2\n",
    "filename = '/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab/images/images/raccoon-100.jpg'\n",
    "unscaled = cv2.imread(filename) # Original image for display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TdRTlbfcAHr"
   },
   "source": [
    "### Resize the image to 128 * 128 and preprocess the image for the MobileNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACVNzBCAcARH"
   },
   "outputs": [],
   "source": [
    "image_height, image_width, _ = unscaled.shape\n",
    "image = cv2.resize(unscaled, (IMAGE_SIZE, IMAGE_SIZE)) # Rescaled image to run the network\n",
    "feat_scaled = preprocess_input(np.array(image, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7Wc1mXlcFT2"
   },
   "source": [
    "### Predict the coordinates of the bounding box for the given test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw2L2wtXcFcH"
   },
   "outputs": [],
   "source": [
    "region = model.predict(x=np.array([feat_scaled]))[0] # Predict the BBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlmhWQe4c6OK"
   },
   "outputs": [],
   "source": [
    "x0 = int(region[0] * image_width / IMAGE_SIZE) # Scale the BBox\n",
    "y0 = int(region[1] * image_height / IMAGE_SIZE)\n",
    "\n",
    "x1 = int((region[0] + region[2]) * image_width / IMAGE_SIZE)\n",
    "y1 = int((region[1] + region[3]) * image_height / IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eF1iqyc4cHm-"
   },
   "source": [
    "### Plot the test image using .imshow and draw a boundary box around the image with the coordinates obtained from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOOfwVgeco-j"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BE0l5FCQcH6p"
   },
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(unscaled)\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgSbStfqcK7x"
   },
   "source": [
    "## Problem 2 : Time Series Analysis using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYBP3G0xctVK"
   },
   "source": [
    "### Download Data\n",
    "Link: https://datamarket.com/data/set/2324/daily-minimum-temperatures-in-melbourne-australia-1981-1990#!ds=2324&display=line\n",
    "\n",
    "#### Description\n",
    "Daily minimum temperatures in Melbourne, Australia, 1981-1990\n",
    "\n",
    "Units: Degrees Celcius\n",
    "\n",
    "#### Steps before loading\n",
    "- Rename the column name with temperature values to \"Temperature\"\n",
    "- In the last, there is one extra row in the data, remove it by opening the file and save it again.\n",
    "- There are some values in Temperature column which have a \"?\" before them, they will give error, remove \"?\" before them and save the file\n",
    "- If you don't want to do these steps, just load the data file given by Great Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaPURhlmcz7b"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sF6yJZRc0Eb"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras  import callbacks\n",
    "from keras import optimizers\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zxykhpelc2ap"
   },
   "source": [
    "#### Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyGmrgDXc2j8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7YJZAdjfSqF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_ShtSc3fcaF"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbzhgaP-c5h3"
   },
   "source": [
    "### Load the  data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6fqtQ3lc6m0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/AIML/Residency 9 (19-21 Apr 2019)/Lab/daily-minimum-temperatures-in-me.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8G1e-yh7g2QF"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YR__nGHGc9HP"
   },
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioeODEwAc9Rg"
   },
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7JhKcDtetJF"
   },
   "source": [
    "### Check for null values and treat them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uT70jzDUeu5T"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvLCAaRIhjXq"
   },
   "outputs": [],
   "source": [
    "# Null Values?\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1RIiJXjexn4"
   },
   "source": [
    "#### Drop null values (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9ru89laezAC"
   },
   "outputs": [],
   "source": [
    "# no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xus3ikVie04Z"
   },
   "source": [
    "### Get the representation of the distribution of data in the form of histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlHV8H9Oe1CN"
   },
   "outputs": [],
   "source": [
    "df.hist(bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXZerWsve5Oq"
   },
   "source": [
    "### Check the maximum and minimum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1DAqyvxe6UM"
   },
   "outputs": [],
   "source": [
    "# Scaled Data\n",
    "print('Min', np.min(df))\n",
    "print('Max', np.max(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozpHcHnDe-oI"
   },
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgWGUM15e_E_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENBvv2YxfB6G"
   },
   "source": [
    "### Check the maximum and minimum values of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7M6hP8GVfCDO"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BI0n8ILdmMzX"
   },
   "outputs": [],
   "source": [
    "print('Min', np.min(scaled))\n",
    "print('Max', np.max(scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8uytX2YfFTP"
   },
   "source": [
    "### Split data into Training and Testing set \n",
    "<u>Note</u> The train and test data has to be sequential. Keep the first 70% of data in training and the bottom 30% in test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pA_L_w5yfGDf"
   },
   "outputs": [],
   "source": [
    "#Split data into Training and Testing se\n",
    "train_size = int(len(scaled) * 0.70)\n",
    "test_size = len(scaled - train_size)\n",
    "train, test = scaled[0:train_size, :], scaled[train_size: len(scaled), :]\n",
    "print('train: {}\\ntest: {}'.format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_j1Qb4QvfJkN"
   },
   "source": [
    "## Create the sequential data\n",
    "Map the temperature at a particular time t to the temperature at time t+n, where n is any number you define.\n",
    "\n",
    "For example: to map temperatures of consecutive days, use t+1, i.e. loop_back = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwu_o3-cfWFj"
   },
   "source": [
    "#### Define your function to create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5Xlj7aOfWPS"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    print(len(dataset), look_back)\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        print(i)\n",
    "        print('X {} to {}'.format(i, i+look_back))\n",
    "        print(a)\n",
    "        print('Y {}'.format(i + look_back))\n",
    "        print(dataset[i + look_back, 0])\n",
    "        dataset[i + look_back, 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7I0D7kNfbbq"
   },
   "source": [
    "### Use function to get training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xssfbRBGfcRq"
   },
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train, look_back)\n",
    "X_test, y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohF6nTWFffYx"
   },
   "source": [
    "### Transform the prepared train and test input data into the expected structure using numpy.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIa99CH4fgG5"
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzM5fu6st3Vy"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmR05XHzt8Ch"
   },
   "outputs": [],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BGTsb6UtcYf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5C4HGteWuAqD"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6kI_q56fi_g"
   },
   "source": [
    "### Define sequntial model, add LSTM layer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZzZfxh2fjIQ"
   },
   "outputs": [],
   "source": [
    "#The network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. \n",
    "#The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs and a batch size of 1 is used.\n",
    "#create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CJCSAl5s1yO"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mubucWirs6bY"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAEu4kGmfnO1"
   },
   "source": [
    "### Train the model for 100 epochs with batch size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YS9480zfnZG"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=15, batch_size=batch_size, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8oYd0wKfrPm"
   },
   "source": [
    "### Make Predictions and Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "e-9v4dqS0sTH",
    "outputId": "94a22428-2d2a-47e0-a928-8488e555c45a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553,)"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hr_ksBwvfrZ8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "trainPredict = model.predict(X_train, batch_size=batch_size)\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "testPredict = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "y_train = scaler.inverse_transform([y_train])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "y_test = scaler.inverse_transform([y_test])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "COzbx5692BwB",
    "outputId": "b7e89632-b5bb-451d-ce09-0a70456ed930"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZeaX2RIfx4Y"
   },
   "outputs": [],
   "source": [
    "trainPredictPlot = np.empty_like(scaled)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(scaled)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(scaled)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(scaler.inverse_transform(scaled))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7qbmvHr1UUR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R9_Internal_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
