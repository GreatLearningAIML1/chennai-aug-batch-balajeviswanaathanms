{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"R9_Advacned_Computer_Vision_FACE_DETECTION_project1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1JDkyazXqt9m","colab_type":"text"},"source":["Advance Computer Vision - Project 1\n","Due May 12 by 11pm  Points 30  Submitting a website url Available Apr 26 at 8am - May 14 at 11pm 19 days\n","This assignment was locked May 14 at 11pm.\n","Dear Participant,\n","\n","We welcome you all to the case-based project of this course. The case study (described below - 30 points) covers concepts taught in Part 1 (First 8 hours of ACV and transfer learning).\n","\n","Question : FACE_DETECTION_Questions.ipynbView in a new window\n","\n","PFB the link to the project brief:\n","\n","ACV_Project1_FaceDetector_Project_Brief.pdfView in a new window\n","\n","Link to the dataset & files:\n","\n","https://drive.google.com/drive/folders/1C52IycU5w87CYq5kJDiZXEa0fXRlXr88?usp=sharing (Links to an external site.)Links to an external site.\n","\n"," \n","\n","Github Link for the submission:  https://classroom.github.com/a/WSWMHTtWLinks to an external site.\n","\n","Regards\n","\n","Program Office"]},{"cell_type":"markdown","metadata":{"id":"IgJNvpw9SnZq","colab_type":"text"},"source":["#### In this problem we use \"Transfer Learning\" of an Object Detector model to detect any object according to the problem in hand.\n","\n","Here, We are particularly interested in detecting faces in a given image.\n","\n","#### To use the model first, we need to import the model and its supporting files for the model to function. \n","\n","We see the below steps to import the model."]},{"cell_type":"code","metadata":{"id":"3wCZYj1zxUm3","colab_type":"code","outputId":"3d08f71b-eac3-48d4-c8c5-2b18418cc0ca","executionInfo":{"status":"ok","timestamp":1564278062337,"user_tz":-330,"elapsed":5627,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["!pip install numpy==1.16.1"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python2.7/dist-packages (1.16.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HtnH8CujQfxM","colab_type":"text"},"source":["### Import MobileNet model given in file `mn_model.py`"]},{"cell_type":"code","metadata":{"id":"6X6dUgnRtjns","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwIN9fVIsT37","colab_type":"code","outputId":"7e94e6be-69de-4aaf-c4ad-7e663db11e08","executionInfo":{"status":"ok","timestamp":1564278066455,"user_tz":-330,"elapsed":1131,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FtnDz1qkQESM","colab_type":"code","colab":{}},"source":["### Import MobileNet model given in file `mn_model.py`\n","\n","os.chdir('/content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/')\n","\n","from mn_model import mn_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOjMXnDvt6aY","colab_type":"code","outputId":"47c0cff7-f88e-4491-f316-70f5907a92a3","executionInfo":{"status":"ok","timestamp":1564278075924,"user_tz":-330,"elapsed":3696,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":251}},"source":["!ls"],"execution_count":50,"outputs":[{"output_type":"stream","text":["dataset\t\t\t\t mobilenet_1_0_224_tf.h5\n","_DS_Store\t\t\t output_test\n","face_generator.py\t\t predictor.py\n","face_generator.pyc\t\t ssd_box_encode_decode_utils.py\n","_gitattributes\t\t\t ssd_box_encode_decode_utils.pyc\n","_gitignore\t\t\t ssd_mobilenet_face_epoch_25_loss0.0916.h5\n","keras_layer_AnchorBoxes.py\t wider_extract.py\n","keras_layer_AnchorBoxes.pyc\t WIDER_train\n","keras_layer_L2Normalization.py\t wider_train_small.npy\n","keras_layer_L2Normalization.pyc  WIDER_train.zip\n","keras_ssd_loss.py\t\t WIDER_val\n","keras_ssd_loss.pyc\t\t wider_val_small.npy\n","mn_model.py\t\t\t WIDER_val.zip\n","mn_model.pyc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dwNy4u8zQk1H","colab_type":"text"},"source":["### Import the BatchGenerator and SSDLoss functions in given files `face_generator.py`, `keras_ssd_loss` and `ssd_box_encode_decode_utils.py` as well, used in MobileNet model"]},{"cell_type":"code","metadata":{"id":"3gVMOicwQlg_","colab_type":"code","colab":{}},"source":["#### Import the BatchGenerator and SSDLoss functions as well, used in MobileNet model\n","\n","from face_generator import BatchGenerator\n","from keras_ssd_loss import SSDLoss\n","from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkzbMq9UeuCM","colab_type":"code","colab":{}},"source":["\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from keras.optimizers import Adam, SGD, Nadam\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, LearningRateScheduler\n","from keras.callbacks import Callback\n","from keras import backend as K \n","from keras.models import load_model\n","from math import ceil \n","import numpy as np \n","from termcolor import colored\n","\n","from mn_model import mn_model\n","from face_generator import BatchGenerator\n","from keras_ssd_loss import SSDLoss\n","from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n","\n","# training parameters\n","from keras import backend as K\n","import scipy.misc as sm\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqyWOSGJRB18","colab_type":"text"},"source":["## Set the parameters for the model\n","\n","#### We need to customize the model parameters according to our problem as given below.\n","\n","#### Set n_classes (no.of classes) = 2, as we are interested in only face detection. \n","#### `Face` will be one class and everything else comes under other class (we can call it as `background`)\n","\n","#### Set class_names = [\"background\", \"face\"]"]},{"cell_type":"code","metadata":{"id":"bcoBM5wlfHgZ","colab_type":"code","colab":{}},"source":["img_height =512\n","img_width = 512\n","\n","img_channels = 3\n","\n","n_classes =2 \n","class_names = [\"background\",\"face\"]\n","\n","scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n","aspect_ratios = [[0.5, 1.0, 2.0],\n","                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n","                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n","                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n","                 [0.5, 1.0, 2.0],\n","                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n","two_boxes_for_ar1 = True\n","limit_boxes = True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n","variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n","coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n","normalize_coords = True\n","\n","det_model_path = \"./\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7vX1GJjTg0v","colab_type":"text"},"source":["### Now, we have imported the model and its dependencies. The next thing is to import the dataset for the model to train on. For this, we are using the WIDER FACE dataset. \n","\n","#### To make the dataset available follow the steps given below.\n","\n","\n","1. Create a folder in your google drive for this project. \n","\n","2. Download the train and test dataset files given in .zip format into your drive folder you created for the project in step-1.\n","\n","3. Set the project path variable according to the folders you created to use for this project in your google drive. \n","\n","      `project_path = \"/content/drive/My Drive/DLCP/\"`\n","\n","4. Now, as we mount the drive the images will be available to use for training and testing but in zip format.\n","\n","5. So, lets extract the images from the zipfiles by using the code given of zipfile module.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"KM1QVQ2pduPE","colab_type":"code","colab":{}},"source":["project_path = '/content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-pOMEK8pCPS","colab_type":"code","colab":{}},"source":["train_images_path = project_path + 'WIDER_train.zip'\n","test_images_path = project_path + 'WIDER_val.zip'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uIDTr5ApCR7","colab_type":"code","colab":{}},"source":["import zipfile\n","archive = zipfile.ZipFile(train_images_path, 'r')\n","archive.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhqDPEEfpCUr","colab_type":"code","colab":{}},"source":["archive = zipfile.ZipFile(test_images_path, 'r')\n","archive.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-iCibT8AYiJ7","colab_type":"text"},"source":["### Now, the images are available. The next thing we need is to get the labels for these images, so that we can use this information while training for detecting faces with the given model using transfer learning. \n","\n","#### Follow the below steps to get those labels available.\n","\n","\n","Load the  '' `wider_train_small.npy`'' file given to check the information given about the dataset. In this file you can see the information about each image in the dataset in a list with following elemets:\n","      \n","\n","        1.   Image filename (str)\n","        2.   Image filename (str)\n","        3.   Image size (list) [height, width]\n","        4.   List of bounding box co-ordinates and Class label (list) [[a,b,c,d], Class label, ...]\n","        \n","        where,\n","        a,b,c,d are the four co-ordinates of the bounding box\n","        Class label is the position of object as mentioned in `class_names` list above."]},{"cell_type":"code","metadata":{"id":"cQe-Z9r_xdTA","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGS751PnwUoz","colab_type":"code","colab":{}},"source":["data = np.load('/content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/wider_train_small.npy', allow_pickle=True).item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DB1gykflwVrr","colab_type":"code","colab":{}},"source":["data_test = np.load('/content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/wider_val_small.npy', allow_pickle=True).item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2Fd8RY3pCJS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"3cd4d5fa-b378-4003-9a2d-b7dba016ebec","executionInfo":{"status":"ok","timestamp":1564278105153,"user_tz":-330,"elapsed":944,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}}},"source":["### Printed first element to check the above given information.\n","\n","for key in data:\n","    print key\n","    print data[key]\n","    break"],"execution_count":59,"outputs":[{"output_type":"stream","text":["52--Photographers/52_Photographers_photographertakingphoto_52_582.jpg\n","['WIDER_train/images/52--Photographers/52_Photographers_photographertakingphoto_52_582.jpg', 'WIDER_train/images/52--Photographers/52_Photographers_photographertakingphoto_52_582.jpg', [300, 300], [[21, 667, 9, 655], 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vlTpXziHZwws","colab_type":"text"},"source":[" As we can see from the above output all the information mentioned above is there for all the images."]},{"cell_type":"markdown","metadata":{"id":"m2NBAZGqaec9","colab_type":"text"},"source":["### Now, load the files `wider_train.npy` and `wider_val.npy`"]},{"cell_type":"code","metadata":{"id":"vA13zmywzSp1","colab_type":"code","colab":{}},"source":["train_data = 'wider_train_small.npy'\n","test_data = 'wider_val_small.npy'\n","\n","x = np.load(train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kfX49vXas4f","colab_type":"code","colab":{}},"source":["train_data = 'wider_train_small.npy'\n","test_data = 'wider_val_small.npy'\n","\n","x = np.load(train_data)\n","x_t = np.load(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2KdmbvGblC3","colab_type":"text"},"source":["### Now, call the imported model with the given parameters and freeze all the layers in the model with names not having ''`detection`'' word as prefix.\n","\n","As we are not training the model from scratch, we are freezing all the above layers in the model having only last few layers while training to update their weights according to the problem in hand. This is called as **Transfer Learning**."]},{"cell_type":"code","metadata":{"id":"ZHFniaSn8OFJ","colab_type":"code","colab":{}},"source":["from keras import backend as K"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHwplT2ggR-1","colab_type":"code","outputId":"dc28e066-208f-4385-c403-ac0dd2fb0f6b","executionInfo":{"status":"ok","timestamp":1564278128750,"user_tz":-330,"elapsed":7802,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# build the keras model\n","# this model is not retrained, we are doing it from scratch \n","\n","K.clear_session()\n","model, model_layer, img_input, predictor_sizes = mn_model(image_size=(img_height, img_width, img_channels), \n","                                                                      n_classes = n_classes,\n","                                                                      min_scale = None, \n","                                                                      max_scale = None, \n","                                                                      scales = scales, \n","                                                                      aspect_ratios_global = None, \n","                                                                      aspect_ratios_per_layer = aspect_ratios, \n","                                                                      two_boxes_for_ar1= two_boxes_for_ar1, \n","                                                                      limit_boxes=limit_boxes, \n","                                                                      variances= variances, \n","                                                                      coords=coords, \n","                                                                      normalize_coords=normalize_coords)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["====> Model Specific data\n","====> Height, Width, Channels : 512 512 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FfC4yNb_uCdT","colab_type":"text"},"source":["#### Write code to freeze all the layers in the above model with names not having ''`detection`'' word as prefix."]},{"cell_type":"code","metadata":{"id":"snOulB0wt7_t","colab_type":"code","outputId":"7beb6ea3-f6ee-4deb-f5ec-1d2497c242fb","executionInfo":{"status":"ok","timestamp":1564278184358,"user_tz":-330,"elapsed":1128,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Freezing layers in the model which don't have 'dense' in their name\n","for layer in model.layers:\n","  if('detection' not in layer.name): #prefix detection to freeze layers which does not have dense\n","    #Freezing a layer\n","    layer.trainable = False\n","\n","#Module to print colourful statements\n","from termcolor import colored\n","\n","#Check which layers have been frozen \n","for layer in model.layers:\n","  print (colored(layer.name, 'blue'))\n","  print (colored(layer.trainable, 'red'))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["\u001b[34minput_1\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mlambda1\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mlambda2\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mlambda3\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv1\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv1_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv1_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_1\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_1_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_1_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_1\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_1_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_1_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_2\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_2_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_2_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_2\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_2_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_2_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_3\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_3_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_3_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_3\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_3_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_3_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_4\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_4_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_4_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_4\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_4_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_4_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_5\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_5_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_5_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_5\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_5_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_5_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_6\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_6_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_6_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_6\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_6_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_6_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_7\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_7_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_7_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_7\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_7_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_7_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_8\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_8_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_8_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_8\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_8_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_8_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_9\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_9_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_9_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_9\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_9_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_9_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_10\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_10_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_10_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_10\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_10_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_10_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_11\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_11_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_11_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_11\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_11_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_11_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_12\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_12_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_12_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_12\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_12_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_12_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_13\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_13_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_dw_13_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_13\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_13_bn\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_pw_13_relu\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mdetection_conv6_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_1_nonlin\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_conv_dw_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_conv_dw_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_conv_dw_1_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_conv_pw_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_conv_pw_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_conv_pw_1_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_1_nonlin\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_conv_dw_2\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_conv_dw_2_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_conv_dw_2_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_conv_pw_2\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_conv_pw_2_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_conv_pw_2_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_1_nonlin\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_conv_dw_3\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_conv_dw_3_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_conv_dw_3_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_conv_pw_3\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_conv_pw_3_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_conv_pw_3_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_1_nonlin\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_conv_dw_4\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_conv_dw_4_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_conv_dw_4_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_conv_pw_4\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_conv_pw_4_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_conv_pw_4_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_conv_dw_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_conv_dw_2\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_conv_dw_3\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_conv_dw_4\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_conv_dw_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_conv_dw_6\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_conv_dw_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_conv_dw_2_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_conv_dw_3_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_conv_dw_4_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_conv_dw_5_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_conv_dw_6_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_conv_dw_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_conv_dw_2\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_conv_dw_3\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_conv_dw_4\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_conv_dw_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_conv_dw_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_conv_dw_1_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_conv_dw_2_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_conv_dw_3_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_conv_dw_4_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_conv_dw_5_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_conv_dw_6_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_conv_dw_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_conv_dw_2_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_conv_dw_3_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_conv_dw_4_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_conv_dw_5_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_conv_dw_5_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_conv_pw_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_conv_pw_2\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_conv_pw_3\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_conv_pw_4\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_conv_pw_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_conv_pw_6\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_conv_dw_1_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_conv_dw_2_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_conv_dw_3_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_conv_dw_4_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_conv_dw_5_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_conv_dw_5_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_conv_pw_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_conv_pw_2_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_conv_pw_3_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_conv_pw_4_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_conv_pw_5_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_conv_pw_6_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_conv_pw_1\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_conv_pw_2\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_conv_pw_3\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_conv_pw_4\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_conv_pw_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_conv_pw_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_conv_pw_1_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_conv_pw_2_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_conv_pw_3_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_conv_pw_4_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_conv_pw_5_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_conv_pw_6_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_conv_pw_1_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_conv_pw_2_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_conv_pw_3_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_conv_pw_4_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_conv_pw_5_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_conv_pw_5_bn\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_conf_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_conf_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_conf_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_conf_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_conf_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_conf_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_conv_pw_1_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_conv_pw_2_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_conv_pw_3_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_conv_pw_4_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_conv_pw_5_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_conv_pw_5_relu\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_mbox_conf\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_loc_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_loc_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_loc_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_loc_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_loc_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_loc_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv4_3_norm_mbox_priorbox_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_fc7_mbox_priorbox_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv6_2_mbox_priorbox_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv7_2_mbox_priorbox_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv8_2_mbox_priorbox_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_conv9_2_mbox_priorbox_reshape\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_mbox_conf_softmax\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_mbox_loc\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_mbox_priorbox\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mdetection_predictions\u001b[0m\n","\u001b[31mTrue\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n1IIGWM2c3x-","colab_type":"text"},"source":["### After making the model ready for transfer learning, load the weights of the model given in file ''`mobilenet_1_0_224_tf.h5`''"]},{"cell_type":"code","metadata":{"id":"lmCmRr2Rc2Sv","colab_type":"code","colab":{}},"source":["#The pre-trained weights must exist in a folder in the current folder\n","model.load_weights('//content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/mobilenet_1_0_224_tf.h5', by_name=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73KNzDSCf6Rh","colab_type":"text"},"source":["#### Using the functions given in the model, we are trying to divide the dataset into train and validation samples. Run the below code."]},{"cell_type":"code","metadata":{"id":"4_pZEU8TfBoR","colab_type":"code","outputId":"d5fd8ae0-1fd0-452c-c6ef-bc9754a6c311","executionInfo":{"status":"ok","timestamp":1564278258250,"user_tz":-330,"elapsed":4172,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["batch_size = 32\n","ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n","                                img_width=img_width,\n","                                n_classes=n_classes, \n","                                predictor_sizes=predictor_sizes,\n","                                min_scale=None,\n","                                max_scale=None,\n","                                scales=scales,\n","                                aspect_ratios_global=None,\n","                                aspect_ratios_per_layer=aspect_ratios,\n","                                two_boxes_for_ar1=two_boxes_for_ar1,\n","                                limit_boxes=limit_boxes,\n","                                variances=variances,\n","                                pos_iou_threshold=0.5,\n","                                neg_iou_threshold=0.2,\n","                                coords=coords,\n","                                normalize_coords=normalize_coords)\n","\n","train_dataset = BatchGenerator(images_path=train_data, \n","                include_classes='all', \n","                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n","\n","print (\"==>TRAINING DATA\")\n","print (\"==> Parsing XML files ...\")\n","\n","train_dataset.parse_xml(\n","                  annotations_path=train_data,\n","                  image_set_path='None',\n","                  image_set='None',\n","                  classes = class_names, \n","                  exclude_truncated=False,\n","                  exclude_difficult=False,\n","                  ret=False, \n","                  debug = False)\n","print(\"==>Parsing XML Finished.\")\n","\n","print (\"==>Generate training batches...\")\n","train_generator = train_dataset.generate(\n","                 batch_size=batch_size,\n","                 train=True,\n","                 ssd_box_encoder=ssd_box_encoder,\n","                 equalize=True,\n","                 brightness=(0.5,2,0.5),\n","                 flip=0.5,\n","                 translate=((0, 20), (0, 30), 0.5),\n","                 scale=(0.75, 1.2, 0.5),\n","                 crop=False,\n","                 #random_crop = (img_height,img_width,1,3), \n","                 random_crop=False,\n","                 resize=(img_height, img_width),\n","                 #resize=False,\n","                 gray=False,\n","                 limit_boxes=True,\n","                 include_thresh=0.4,\n","                 diagnostics=False)\n","\n","print (\"==>Training batch generation complete\")\n","\n","n_train_samples = train_dataset.get_n_samples()\n","\n","print (\"==>Total number of training samples = {}\".format(n_train_samples))\n","\n","# Now repeat above steps for validation data \n","\n","print (\"==>VALIDATION\")\n","\n","val_dataset = BatchGenerator(images_path=test_data, include_classes='all', \n","                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n","\n","print (\"==> Parsing XML files ...\")\n","\n","\n","val_dataset.parse_xml(\n","                  annotations_path=test_data,\n","                  image_set_path='None',\n","                  image_set='None',\n","                  classes = class_names, \n","                  exclude_truncated=False,\n","                  exclude_difficult=False,\n","                  ret=False, \n","                  debug = False)\n","\n","\n","print(\"==>Parsing XML Finished.\")\n","\n","\n","print (\"==>Generate training batches...\")\n","val_generator = val_dataset.generate(\n","                 batch_size=batch_size,\n","                 train=True,\n","                 ssd_box_encoder=ssd_box_encoder,\n","                 equalize=False,\n","                 brightness=False,\n","                 flip=False,\n","                 translate=False,\n","                 scale=False,\n","                 crop=False,\n","                 #random_crop = (img_height,img_width,1,3), \n","                 random_crop=False, \n","                 resize=(img_height, img_width), \n","                 #resize=False, \n","                 gray=False,\n","                 limit_boxes=True,\n","                 include_thresh=0.4,\n","                 diagnostics=False)\n","\n","\n","print (\"==>Training batch generation complete\")\n","\n","n_val_samples = val_dataset.get_n_samples()\n","\n","print (\"==>Total number of validation samples = {}\".format(n_val_samples))"],"execution_count":69,"outputs":[{"output_type":"stream","text":["==>TRAINING DATA\n","==> Parsing XML files ...\n","==>Parsing XML Finished.\n","==>Generate training batches...\n","==>Training batch generation complete\n","==>Total number of training samples = 128\n","==>VALIDATION\n","==> Parsing XML files ...\n","==>Parsing XML Finished.\n","==>Generate training batches...\n","==>Training batch generation complete\n","==>Total number of validation samples = 60\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4KtkdFTQhKlE","colab_type":"text"},"source":["### Now, lets setup things for training by initilaizing required variables like learning rate, epochs, optimizer and loss function(SSDLoss) to compile the model"]},{"cell_type":"code","metadata":{"id":"g1-3OOPmgmxk","colab_type":"code","colab":{}},"source":["# setting up training \n","\n","# batch_size and no.of epochs\n","\n","batch_size = 16\n","num_epochs = 5\n","\n","#Learning rate\n","base_lr = 0.002\n","\n","# Optimizer\n","adam = Adam(lr=base_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay = 0.0)\n","\n","# Loss\n","ssd_loss = SSDLoss(neg_pos_ratio=2, n_neg_min=0, alpha=1.0, beta = 1.0)\n","\n","# Compile\n","model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vt_RxSHJitPR","colab_type":"text"},"source":["### Lets add early stopping and model checkpoint layers on validation loss with some patience values and using fit_generator function to train the model on data generated batch-by-batch by a Python generator, `train_generator` object as generator.\n","\n","\n","We are using checkpoint to save the best model based on validation accuracy."]},{"cell_type":"markdown","metadata":{"id":"ugJ68M7_vDqy","colab_type":"text"},"source":["#### Write code for early_stopping and model_checkpoint layers. Using model.fit_generator train the model and save the best weight file."]},{"cell_type":"code","metadata":{"id":"EeO-T9jwi8be","colab_type":"code","outputId":"061a81a3-cb40-4391-b320-8db67e6e3589","executionInfo":{"status":"ok","timestamp":1564279302086,"user_tz":-330,"elapsed":964532,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["\n","filepath = '//content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/mobilenet_1_0_224_tf.h5'\n","\n","checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True,\n","                             save_weights_only=False, mode=\"min\", period=1) # Checkpoint best validation model\n","\n","stop = EarlyStopping(monitor=\"ssd_loss\", patience=7, mode=\"min\") # Stop early, if the validation error deteriorates\n","\n","#reduce_lr = ReduceLROnPlateau(monitor=\"ssd_loss\", factor=0.2, patience=10, min_lr=1e-7, verbose=1, mode=\"max\")\n","\n","callback_list = [checkpoint, stop]\n","\n","model.fit_generator(train_generator, nb_epoch=num_epochs, steps_per_epoch=10, callbacks = callback_list)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","10/10 [==============================] - 211s 21s/step - loss: 0.3420\n","Epoch 2/5\n","10/10 [==============================] - 186s 19s/step - loss: 0.2776\n","Epoch 3/5\n","10/10 [==============================] - 187s 19s/step - loss: 0.2512\n","Epoch 4/5\n","10/10 [==============================] - 187s 19s/step - loss: 0.2344\n","Epoch 5/5\n","10/10 [==============================] - 187s 19s/step - loss: 0.2277\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fecd6f25050>"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"markdown","metadata":{"id":"U3FHrp77jdOx","colab_type":"text"},"source":["### Load the best saved model from above step and check predictions for test data using test_generator object to generate batches."]},{"cell_type":"markdown","metadata":{"id":"S7eP30cxvcEA","colab_type":"text"},"source":["#### Write code in the below cell to load best saved model in the above step."]},{"cell_type":"code","metadata":{"id":"nh7tMrxFjR4B","colab_type":"code","colab":{}},"source":["model_path = '/content/drive/My Drive/R9 Advanced Computer Vision Project 1/Face Detection/'\n","\n","model_name = 'ssd_mobilenet_face_epoch_25_loss0.0916.h5'\n","\n","model.load_weights(model_path + model_name,  by_name= True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dD7q7pzqvnh","colab_type":"text"},"source":["### Use the below function to plot the boundingbox in the test image to show the predictions."]},{"cell_type":"code","metadata":{"id":"h89fe5NGqwI9","colab_type":"code","colab":{}},"source":["def save_bb(path, filename, results, prediction=True):\n","  \n","  # print filename\n","\n","  img = image.load_img(filename, target_size=(img_height, img_width))\n","  img = image.img_to_array(img)\n","\n","  filename = filename.split(\"/\")[-1]\n","\n","  if(not prediction):\n","    filename = filename[:-4] + \"_gt\" + \".jpg\"\n","\n","  #fig,currentAxis = plt.subplots(1)\n","  currentAxis = plt.gca()\n","\n"," # Get detections with confidence higher than 0.6.\n","  colors = plt.cm.hsv(np.linspace(0, 1, 25)).tolist()\n","  color_code = min(len(results), 16)\n","  print (colored(\"total number of bbs: %d\" % len(results), \"yellow\"))\n","  for result in results:\n","    # Parse the outputs.\n","\n","    if(prediction):\n","      det_label = result[0]\n","      det_conf = result[1]\n","      det_xmin = result[2]\n","      det_xmax = result[3]\n","      det_ymin = result[4]\n","      det_ymax = result[5]\n","    else :\n","      det_label = result[0]\n","      det_xmin = result[1]\n","      det_xmax = result[2]\n","      det_ymin = result[3]\n","      det_ymax = result[4]\n","\n","    xmin = int(det_xmin)\n","    ymin = int(det_ymin)\n","    xmax = int(det_xmax)\n","    ymax = int(det_ymax)\n","\n","    if(prediction):\n","      score = det_conf\n","    \n","    plt.imshow(img / 255.)\n","    \n","    label = int(int(det_label))\n","    label_name = class_names[label]\n","    # print label_name \n","    # print label\n","\n","    if(prediction):\n","      display_txt = '{:0.2f}'.format(score)\n","    else:\n","      display_txt = '{}'.format(label_name)\n","\n","      \n","    # print (xmin, ymin, ymin, ymax)\n","    coords = (xmin, ymin), (xmax-xmin), (ymax-ymin)\n","    color_code = color_code-1 \n","    color = colors[color_code]\n","    currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n","    currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.2})\n","\n","  # y\n","  currentAxis.axes.get_yaxis().set_visible(False)\n","  # x\n","  currentAxis.axes.get_xaxis().set_visible(False)\n","  plt.savefig(path + filename, bbox_inches='tight')\n","\n","  print ('saved' , path + filename)\n","\n","  plt.clf()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrMdNhMtwE3t","colab_type":"text"},"source":["#### Run the below code to create a folder with name output_test and get the predictions for the test images using model.predict()"]},{"cell_type":"code","metadata":{"id":"1Sep0lG3vR0R","colab_type":"code","outputId":"d8c60daa-b7a2-42f0-833f-672f7fe70c30","executionInfo":{"status":"ok","timestamp":1564280111875,"user_tz":-330,"elapsed":4005,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["!mkdir output_test"],"execution_count":79,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘output_test’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"70s6RPuInpqo","colab_type":"code","outputId":"dd8137b0-9240-431f-a492-474c0bd08aea","executionInfo":{"status":"error","timestamp":1564280251543,"user_tz":-330,"elapsed":139708,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from keras.preprocessing import image\n","from matplotlib import pyplot as plt\n","\n","test_size = 16\n","test_generator = val_dataset.generate(\n","                 batch_size=test_size,\n","                 train=False,\n","                 ssd_box_encoder=ssd_box_encoder,\n","                 equalize=False,\n","                 brightness=False,\n","                 flip=False,\n","                 translate=False,\n","                 scale=False,\n","                 crop=False,\n","                 #random_crop = (img_height,img_width,1,3), \n","                 random_crop=False, \n","                 resize=(img_height, img_width), \n","                 #resize=False,\n","                 gray=False,\n","                 limit_boxes=True,\n","                 include_thresh=0.4,\n","                 diagnostics=False)\n","\n","print (colored(\"done.\", \"green\"))\n","\n","print (colored(\"now predicting...\", \"yellow\"))\n","\n","_CONF = 0.60 \n","_IOU = 0.15\n","\n","for i in range(test_size):\n","    X, y, filenames = next(test_generator)\n","\n","    y_pred = model.predict(X)\n","    y_pred_decoded = decode_y2(y_pred,\n","                             confidence_thresh=_CONF,\n","                            iou_threshold=_IOU,\n","                            top_k='all',\n","                            input_coords=coords,\n","                            normalize_coords=normalize_coords,\n","                            img_height=img_height,\n","                            img_width=img_width)\n","\n","\n","    np.set_printoptions(suppress=True)\n","\n","    save_bb(\"./output_test/\", filenames[i], y_pred_decoded[i])\n","    save_bb(\"./output_test/\", filenames[i], y[i], prediction=False)\n"],"execution_count":80,"outputs":[{"output_type":"stream","text":["\u001b[32mdone.\u001b[0m\n","\u001b[33mnow predicting...\u001b[0m\n","\u001b[33mtotal number of bbs: 3\u001b[0m\n","('saved', './output_test/2_Demonstration_Demonstration_Or_Protest_2_367.jpg')\n","\u001b[33mtotal number of bbs: 9\u001b[0m\n","('saved', './output_test/2_Demonstration_Demonstration_Or_Protest_2_367_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/30_Surgeons_Surgeons_30_533.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/30_Surgeons_Surgeons_30_533_gt.jpg')\n","\u001b[33mtotal number of bbs: 3\u001b[0m\n","('saved', './output_test/28_Sports_Fan_Sports_Fan_28_590.jpg')\n","\u001b[33mtotal number of bbs: 9\u001b[0m\n","('saved', './output_test/28_Sports_Fan_Sports_Fan_28_590_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/18_Concerts_Concerts_18_102.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/18_Concerts_Concerts_18_102_gt.jpg')\n","\u001b[33mtotal number of bbs: 3\u001b[0m\n","('saved', './output_test/30_Surgeons_Surgeons_30_554.jpg')\n","\u001b[33mtotal number of bbs: 6\u001b[0m\n","('saved', './output_test/30_Surgeons_Surgeons_30_554_gt.jpg')\n","\u001b[33mtotal number of bbs: 0\u001b[0m\n","('saved', './output_test/25_Soldier_Patrol_Soldier_Patrol_25_467.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/25_Soldier_Patrol_Soldier_Patrol_25_467_gt.jpg')\n","\u001b[33mtotal number of bbs: 2\u001b[0m\n","('saved', './output_test/45_Balloonist_Balloonist_45_225.jpg')\n","\u001b[33mtotal number of bbs: 3\u001b[0m\n","('saved', './output_test/45_Balloonist_Balloonist_45_225_gt.jpg')\n","\u001b[33mtotal number of bbs: 5\u001b[0m\n","('saved', './output_test/2_Demonstration_Protesters_2_646.jpg')\n","\u001b[33mtotal number of bbs: 14\u001b[0m\n","('saved', './output_test/2_Demonstration_Protesters_2_646_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/18_Concerts_Concerts_18_102.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/18_Concerts_Concerts_18_102_gt.jpg')\n","\u001b[33mtotal number of bbs: 4\u001b[0m\n","('saved', './output_test/26_Soldier_Drilling_Soldiers_Drilling_26_393.jpg')\n","\u001b[33mtotal number of bbs: 5\u001b[0m\n","('saved', './output_test/26_Soldier_Drilling_Soldiers_Drilling_26_393_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/13_Interview_Interview_Sequences_13_779.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/13_Interview_Interview_Sequences_13_779_gt.jpg')\n","\u001b[33mtotal number of bbs: 3\u001b[0m\n","('saved', './output_test/2_Demonstration_Demonstration_Or_Protest_2_367.jpg')\n","\u001b[33mtotal number of bbs: 9\u001b[0m\n","('saved', './output_test/2_Demonstration_Demonstration_Or_Protest_2_367_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/16_Award_Ceremony_Awards_Ceremony_16_338.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/16_Award_Ceremony_Awards_Ceremony_16_338_gt.jpg')\n","\u001b[33mtotal number of bbs: 0\u001b[0m\n","('saved', './output_test/25_Soldier_Patrol_Soldier_Patrol_25_467.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/25_Soldier_Patrol_Soldier_Patrol_25_467_gt.jpg')\n","\u001b[33mtotal number of bbs: 2\u001b[0m\n","('saved', './output_test/28_Sports_Fan_Sports_Fan_28_86.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/28_Sports_Fan_Sports_Fan_28_86_gt.jpg')\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-80-58d50d612371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuppress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0msave_bb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output_test/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_decoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0msave_bb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output_test/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"qLKgtPCJwena","colab_type":"text"},"source":["In the above step all the test images along with predictions are stored in output_test folder in this notebook environment. You can check the folder in Files section of the menu to left-side of screen in colab.\n","\n","Each test image is used for predictions and is stored as 2 files.\n","\n","one file is the original ground truth with <filename_gt.jpg>\n","second file is the prediction of the model on the image. with <filename.jpg>"]},{"cell_type":"markdown","metadata":{"id":"TFzb3Vj9kDGH","colab_type":"text"},"source":["### Visualize a test image to check predictions\n","\n","\n","#### Write code to show images: Using cv2.imshow() or matplotlib show any 3 test images and their predictions."]},{"cell_type":"code","metadata":{"id":"zA1lMnX4-edG","colab_type":"code","outputId":"d85d2f6b-bfee-4f99-869f-47894def65e6","executionInfo":{"status":"ok","timestamp":1564281438260,"user_tz":-330,"elapsed":5926,"user":{"displayName":"Soham Bhirud","photoUrl":"","userId":"02419327484996831663"}},"colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["from keras.preprocessing import image\n","from matplotlib import pyplot as plt\n","\n","test_size = 3\n","test_generator = val_dataset.generate(\n","                 batch_size=test_size,\n","                 train=False,\n","                 ssd_box_encoder=ssd_box_encoder,\n","                 equalize=False,\n","                 brightness=False,\n","                 flip=False,\n","                 translate=False,\n","                 scale=False,\n","                 crop=False,\n","                 #random_crop = (img_height,img_width,1,3), \n","                 random_crop=False, \n","                 resize=(img_height, img_width), \n","                 #resize=False,\n","                 gray=False,\n","                 limit_boxes=True,\n","                 include_thresh=0.4,\n","                 diagnostics=False)\n","\n","print (colored(\"done.\", \"green\"))\n","\n","print (colored(\"now predicting...\", \"yellow\"))\n","\n","_CONF = 0.60 \n","_IOU = 0.15\n","\n","for i in range(test_size):\n","  X, y, filenames = next(test_generator)\n","\n","  y_pred = model.predict(X)\n","\n","\n","  y_pred_decoded = decode_y2(y_pred,\n","                             confidence_thresh=_CONF,\n","                            iou_threshold=_IOU,\n","                            top_k='all',\n","                            input_coords=coords,\n","                            normalize_coords=normalize_coords,\n","                            img_height=img_height,\n","                            img_width=img_width)\n","\n","\n","  np.set_printoptions(suppress=True)\n","\n","  save_bb(\"./output_test/\", filenames[i], y_pred_decoded[i])\n","  save_bb(\"./output_test/\", filenames[i], y[i], prediction=False)"],"execution_count":81,"outputs":[{"output_type":"stream","text":["\u001b[32mdone.\u001b[0m\n","\u001b[33mnow predicting...\u001b[0m\n","\u001b[33mtotal number of bbs: 0\u001b[0m\n","('saved', './output_test/25_Soldier_Patrol_Soldier_Patrol_25_467.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/25_Soldier_Patrol_Soldier_Patrol_25_467_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/18_Concerts_Concerts_18_102.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/18_Concerts_Concerts_18_102_gt.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/49_Greeting_peoplegreeting_49_589.jpg')\n","\u001b[33mtotal number of bbs: 1\u001b[0m\n","('saved', './output_test/49_Greeting_peoplegreeting_49_589_gt.jpg')\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]}]}